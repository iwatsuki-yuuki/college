{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rTlrRvmk8Eio"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yn86OXeoPaH8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b67451a8-19ac-4bbb-abe0-272f3e234f93"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting optuna\n",
            "  Downloading optuna-4.3.0-py3-none-any.whl.metadata (17 kB)\n",
            "Collecting alembic>=1.5.0 (from optuna)\n",
            "  Downloading alembic-1.15.2-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting colorlog (from optuna)\n",
            "  Downloading colorlog-6.9.0-py3-none-any.whl.metadata (10 kB)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from optuna) (24.2)\n",
            "Requirement already satisfied: sqlalchemy>=1.4.2 in /usr/local/lib/python3.11/dist-packages (from optuna) (2.0.40)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from optuna) (4.67.1)\n",
            "Requirement already satisfied: PyYAML in /usr/local/lib/python3.11/dist-packages (from optuna) (6.0.2)\n",
            "Requirement already satisfied: Mako in /usr/lib/python3/dist-packages (from alembic>=1.5.0->optuna) (1.1.3)\n",
            "Requirement already satisfied: typing-extensions>=4.12 in /usr/local/lib/python3.11/dist-packages (from alembic>=1.5.0->optuna) (4.13.2)\n",
            "Requirement already satisfied: greenlet>=1 in /usr/local/lib/python3.11/dist-packages (from sqlalchemy>=1.4.2->optuna) (3.2.2)\n",
            "Downloading optuna-4.3.0-py3-none-any.whl (386 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m386.6/386.6 kB\u001b[0m \u001b[31m12.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading alembic-1.15.2-py3-none-any.whl (231 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m231.9/231.9 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading colorlog-6.9.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: colorlog, alembic, optuna\n",
            "Successfully installed alembic-1.15.2 colorlog-6.9.0 optuna-4.3.0\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets\n",
        "from torchvision.transforms import ToTensor, Lambda, Compose\n",
        "import matplotlib.pyplot as plt\n",
        "!pip install optuna"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9lWbn3sF8OwS"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "goB3awyhoG9d"
      },
      "outputs": [],
      "source": [
        "from sklearn.preprocessing import StandardScaler"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "--Qzi1c3pKnR",
        "outputId": "5156fa6f-698c-48dd-e9c7-56f8b79fac27"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (2.2.2)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.11/dist-packages (2.18.0)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.11/dist-packages (1.6.1)\n",
            "Requirement already satisfied: numpy>=1.23.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (25.2.10)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from tensorflow) (24.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (5.29.4)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.32.3)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (4.13.2)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.17.2)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (1.71.0)\n",
            "Requirement already satisfied: tensorboard<2.19,>=2.18 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (2.18.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.8.0)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (3.13.0)\n",
            "Requirement already satisfied: ml-dtypes<0.5.0,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.4.1)\n",
            "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /usr/local/lib/python3.11/dist-packages (from tensorflow) (0.37.1)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.15.3)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (1.5.0)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.11/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.0.9)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.11/dist-packages (from keras>=3.5.0->tensorflow) (0.15.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.4.26)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.8)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from tensorboard<2.19,>=2.18->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.11/dist-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow) (3.0.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n"
          ]
        }
      ],
      "source": [
        "# pandasとtensorflowのインストール（必要に応じて）\n",
        "!pip install pandas tensorflow scikit-learn\n",
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "import numpy as np\n",
        "import random as random\n",
        "import os\n",
        "import sys\n",
        "import copy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#乱数の固定化\n",
        "\n",
        "def seed_everything(seed: int = 42):\n",
        "    random.seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
        "\n",
        "    if \"torch\" in sys.modules:\n",
        "        torch.manual_seed(seed)\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "seed_everything(42)"
      ],
      "metadata": {
        "id": "GmuoMf-PiyJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7ZkxjZ1opStb",
        "outputId": "db75857b-e9e7-4de4-ca9f-be32d31b9203"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2025-05-18 18:24:00--  https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
            "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.108.133, 185.199.109.133, 185.199.110.133, ...\n",
            "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.108.133|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 2546489 (2.4M) [text/plain]\n",
            "Saving to: ‘compas-scores-two-years.csv’\n",
            "\n",
            "compas-scores-two-y 100%[===================>]   2.43M  --.-KB/s    in 0.03s   \n",
            "\n",
            "2025-05-18 18:24:00 (85.5 MB/s) - ‘compas-scores-two-years.csv’ saved [2546489/2546489]\n",
            "\n",
            "   id                name   first         last compas_screening_date   sex  \\\n",
            "0   1    miguel hernandez  miguel    hernandez            2013-08-14  Male   \n",
            "1   3         kevon dixon   kevon        dixon            2013-01-27  Male   \n",
            "2   4            ed philo      ed        philo            2013-04-14  Male   \n",
            "3   5         marcu brown   marcu        brown            2013-01-13  Male   \n",
            "4   6  bouthy pierrelouis  bouthy  pierrelouis            2013-03-26  Male   \n",
            "\n",
            "          dob  age          age_cat              race  ...  v_decile_score  \\\n",
            "0  1947-04-18   69  Greater than 45             Other  ...               1   \n",
            "1  1982-01-22   34          25 - 45  African-American  ...               1   \n",
            "2  1991-05-14   24     Less than 25  African-American  ...               3   \n",
            "3  1993-01-21   23     Less than 25  African-American  ...               6   \n",
            "4  1973-01-22   43          25 - 45             Other  ...               1   \n",
            "\n",
            "   v_score_text  v_screening_date  in_custody  out_custody  priors_count.1  \\\n",
            "0           Low        2013-08-14  2014-07-07   2014-07-14               0   \n",
            "1           Low        2013-01-27  2013-01-26   2013-02-05               0   \n",
            "2           Low        2013-04-14  2013-06-16   2013-06-16               4   \n",
            "3        Medium        2013-01-13         NaN          NaN               1   \n",
            "4           Low        2013-03-26         NaN          NaN               2   \n",
            "\n",
            "  start   end event two_year_recid  \n",
            "0     0   327     0              0  \n",
            "1     9   159     1              1  \n",
            "2     0    63     0              1  \n",
            "3     0  1174     0              0  \n",
            "4     0  1102     0              0  \n",
            "\n",
            "[5 rows x 53 columns]\n"
          ]
        }
      ],
      "source": [
        "# Compasデータセットのダウンロード\n",
        "!wget https://raw.githubusercontent.com/propublica/compas-analysis/master/compas-scores-two-years.csv\n",
        "\n",
        "# pandasでデータを読み込む\n",
        "data = pd.read_csv(\"compas-scores-two-years.csv\")\n",
        "\n",
        "# データの最初の5行を表示して確認\n",
        "print(data.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YTYUXj5yHIXY"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6g8a6eMCxrbm",
        "outputId": "f54fdcb6-d27b-479e-b44e-f18d9cb5192a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "NaNを含む列: ['days_b_screening_arrest', 'c_jail_in', 'c_jail_out', 'c_case_number', 'c_offense_date', 'c_arrest_date', 'c_days_from_compas', 'c_charge_desc', 'r_case_number', 'r_charge_degree', 'r_days_from_arrest', 'r_offense_date', 'r_charge_desc', 'r_jail_in', 'r_jail_out', 'violent_recid', 'vr_case_number', 'vr_charge_degree', 'vr_offense_date', 'vr_charge_desc', 'in_custody', 'out_custody']\n"
          ]
        }
      ],
      "source": [
        "# データ（特徴量）をPandasのデータフレームに変換\n",
        "df_data = data\n",
        "# ターゲット（ラベル）を別に保存\n",
        "target = data['two_year_recid']\n",
        "\n",
        "#NaNを含むColumn名\n",
        "nan_columns = df_data.columns[df_data.isnull().any()].tolist()\n",
        "print(\"NaNを含む列:\", nan_columns)\n",
        "\n",
        "\n",
        "#欠損値がある行を削除\n",
        "df_data = data.dropna()\n",
        "# 不要な列を削除（例：idなど）\n",
        "df_data = data.drop(columns=['id', 'first','last','start','end','event'], axis=1)\n",
        "df_data = df_data.drop(nan_columns, axis=1)\n",
        "\n",
        "\n",
        "df_encoded = pd.get_dummies(df_data)\n",
        "\n",
        "# ターゲット（ラベル）をOne-Hot Encoding後のデータフレームに追加\n",
        "df_encoded['target'] = target\n",
        "\n",
        "X = df_encoded.drop(['two_year_recid','is_recid','is_violent_recid','target'], axis=1)  # 特徴量\n",
        "\n",
        "y = df_encoded['target'] if 'target' in df_encoded.columns else df_data['target']  # 'target'が存在する場合のみ取得\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5uuemW8n0JTD"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_zi5FukVK4NY"
      },
      "outputs": [],
      "source": [
        "# 訓練セットとテストセットに分割\n",
        "# (課題1)50％をテストデータとして分けた\n",
        "X_train_val, X_test, y_train_val, y_test = train_test_split(X, y,test_size=0.5, random_state=42)\n",
        "\n",
        "# 残り50%から25%を検証データにする（全体の15%）\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train_val, y_train_val, test_size = 0.50, random_state=42)\n",
        "\n",
        "# データの標準化\n",
        "\n",
        "scaler = StandardScaler()\n",
        "\n",
        "# 訓練データに対して平均と標準偏差を計算し、それで変換する\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "# 訓練データで計算した基準（平均.標準偏差）を使って検証データとテストデータも同じように整える\n",
        "X_val = scaler.fit_transform(X_val)\n",
        "X_test = scaler.transform(X_test)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L37ANqSWDmE9"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WbYZfMkp6PO0"
      },
      "outputs": [],
      "source": [
        "# NumPy配列からPyTorchのテンソルに変換\n",
        "X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
        "y_train_tensor = torch.tensor(y_train.values, dtype=torch.long)  # 二値分類の場合、ラベルはlong型\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ch9cuGvt7Zfo"
      },
      "outputs": [],
      "source": [
        "X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
        "y_val_tensor = torch.tensor(y_val.values, dtype=torch.long)\n",
        "\n",
        "\n",
        "X_test_tensor = torch.tensor(X_test, dtype=torch.float32)\n",
        "y_test_tensor = torch.tensor(y_test.values, dtype=torch.long)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ynYvSxUTQFun"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PUO_fzZSTKwB"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9yDVW9iW0m3v"
      },
      "outputs": [],
      "source": [
        "class CompasDataset:\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1mQ7suUpPaH_",
        "outputId": "17defe66-9f17-4cfc-868a-574359de18b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<__main__.CompasDataset object at 0x7c3f038e6350>\n"
          ]
        }
      ],
      "source": [
        "batch_size = 128\n",
        "train_dataset = CompasDataset(X_train_tensor, y_train_tensor)\n",
        "val_dataset = CompasDataset(X_val_tensor, y_val_tensor)\n",
        "test_dataset = CompasDataset(X_test_tensor, y_test_tensor)\n",
        "# データローダーの作成\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "print(train_dataset)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tDVdF4gZ0fvq"
      },
      "outputs": [],
      "source": [
        "class CompasDataset:\n",
        "    def __init__(self, features, labels):\n",
        "        self.features = features\n",
        "        self.labels = labels  # Assign labels directly instead of calling .values\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.labels)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.features[idx], self.labels[idx] # Access elements using indexing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iPbynTvRoN9j",
        "outputId": "6a79a9c2-41d0-4687-b4ee-cab98a01875d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "14710\n"
          ]
        }
      ],
      "source": [
        "length=len(X.columns)\n",
        "print(length)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LxiGClK2PaIA",
        "outputId": "31665f6e-b97e-4f62-8cb9-29a32617d81d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-18 18:24:07,796] A new study created in memory with name: no-name-3a441b92-628e-44e8-be95-df8dfc3026c9\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[I 2025-05-18 18:24:15,353] Trial 0 finished with value: 0.5177383592017738 and parameters: {'hidden_size': 232, 'dropout_rate': 0.41068047119643336, 'activation': 'ReLU', 'optimizer': 'SGD', 'lr': 0.0013198131606321705}. Best is trial 0 with value: 0.5177383592017738.\n",
            "[I 2025-05-18 18:24:21,966] Trial 1 finished with value: 0.48558758314855877 and parameters: {'hidden_size': 120, 'dropout_rate': 0.4461437067330809, 'activation': 'ReLU', 'optimizer': 'SGD', 'lr': 0.00022371200578435988}. Best is trial 0 with value: 0.5177383592017738.\n",
            "[I 2025-05-18 18:24:32,333] Trial 2 finished with value: 0.6491130820399114 and parameters: {'hidden_size': 457, 'dropout_rate': 0.3634747346968012, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.00043548136501758304}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:24:36,467] Trial 3 finished with value: 0.6413525498891353 and parameters: {'hidden_size': 285, 'dropout_rate': 0.2797541510337412, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.0034520535595694663}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:24:39,658] Trial 4 finished with value: 0.6197339246119734 and parameters: {'hidden_size': 367, 'dropout_rate': 0.008003501733233775, 'activation': 'ReLU', 'optimizer': 'SGD', 'lr': 0.007560203395849987}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:24:42,506] Trial 5 finished with value: 0.5415742793791575 and parameters: {'hidden_size': 78, 'dropout_rate': 0.11545738764348085, 'activation': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.0013859366787541344}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:24:45,219] Trial 6 finished with value: 0.5393569844789357 and parameters: {'hidden_size': 325, 'dropout_rate': 0.2346928336694839, 'activation': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.00038248851557724854}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:24:48,007] Trial 7 finished with value: 0.6468957871396895 and parameters: {'hidden_size': 187, 'dropout_rate': 0.06878529241112646, 'activation': 'ReLU', 'optimizer': 'Adam', 'lr': 0.0026055561731995556}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:24:50,996] Trial 8 finished with value: 0.6241685144124168 and parameters: {'hidden_size': 496, 'dropout_rate': 0.4523166438092971, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.004940616891967986}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:24:53,892] Trial 9 finished with value: 0.6186252771618626 and parameters: {'hidden_size': 71, 'dropout_rate': 0.47267291580700177, 'activation': 'ReLU', 'optimizer': 'Adam', 'lr': 0.0031946517623992164}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:24:56,669] Trial 10 finished with value: 0.6480044345898004 and parameters: {'hidden_size': 487, 'dropout_rate': 0.32535492452012543, 'activation': 'Tanh', 'optimizer': 'Adam', 'lr': 0.00011588166577272227}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:24:59,459] Trial 11 finished with value: 0.6385809312638581 and parameters: {'hidden_size': 512, 'dropout_rate': 0.32398323141502255, 'activation': 'Tanh', 'optimizer': 'Adam', 'lr': 0.00010723050868472266}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:25:02,308] Trial 12 finished with value: 0.6147450110864745 and parameters: {'hidden_size': 416, 'dropout_rate': 0.35526900128126876, 'activation': 'Tanh', 'optimizer': 'Adam', 'lr': 0.00045853557234328164}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:25:05,426] Trial 13 finished with value: 0.6419068736141907 and parameters: {'hidden_size': 431, 'dropout_rate': 0.21297752687515836, 'activation': 'Tanh', 'optimizer': 'Adam', 'lr': 0.00010868947524933582}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:25:08,197] Trial 14 finished with value: 0.6252771618625277 and parameters: {'hidden_size': 437, 'dropout_rate': 0.359444449954546, 'activation': 'Tanh', 'optimizer': 'Adam', 'lr': 0.0005751583086276108}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:25:11,026] Trial 15 finished with value: 0.6480044345898004 and parameters: {'hidden_size': 457, 'dropout_rate': 0.1676472019316224, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.00020940182394697676}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:25:13,777] Trial 16 finished with value: 0.6385809312638581 and parameters: {'hidden_size': 373, 'dropout_rate': 0.2959909272012422, 'activation': 'Tanh', 'optimizer': 'Adam', 'lr': 0.00022056480672043156}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:25:16,920] Trial 17 finished with value: 0.6213968957871396 and parameters: {'hidden_size': 371, 'dropout_rate': 0.3951446506657585, 'activation': 'Tanh', 'optimizer': 'Adam', 'lr': 0.0007851575390094926}. Best is trial 2 with value: 0.6491130820399114.\n",
            "[I 2025-05-18 18:25:19,705] Trial 18 finished with value: 0.6518847006651884 and parameters: {'hidden_size': 469, 'dropout_rate': 0.19783548014228636, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.0003134561841147593}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:22,423] Trial 19 finished with value: 0.6396895787139689 and parameters: {'hidden_size': 288, 'dropout_rate': 0.16805674021661798, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.00029678365178932266}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:25,180] Trial 20 finished with value: 0.635809312638581 and parameters: {'hidden_size': 329, 'dropout_rate': 0.16558961682938006, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.0007884878164802747}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:28,276] Trial 21 finished with value: 0.6485587583148559 and parameters: {'hidden_size': 475, 'dropout_rate': 0.25962264390724477, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.0001534408277410562}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:31,067] Trial 22 finished with value: 0.6380266075388027 and parameters: {'hidden_size': 461, 'dropout_rate': 0.25368814726160965, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.00015568065731368547}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:33,818] Trial 23 finished with value: 0.6480044345898004 and parameters: {'hidden_size': 405, 'dropout_rate': 0.19812660933038917, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.0003303370012437077}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:36,626] Trial 24 finished with value: 0.6480044345898004 and parameters: {'hidden_size': 475, 'dropout_rate': 0.25997379413303, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.0005746106608174496}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:39,827] Trial 25 finished with value: 0.643569844789357 and parameters: {'hidden_size': 385, 'dropout_rate': 0.1362580149025331, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.00015931923822895213}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:43,737] Trial 26 finished with value: 0.49722838137472286 and parameters: {'hidden_size': 440, 'dropout_rate': 0.0878365175149546, 'activation': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.0002905973334096562}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:46,502] Trial 27 finished with value: 0.6446784922394678 and parameters: {'hidden_size': 504, 'dropout_rate': 0.3144813958957396, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.0001652811567132679}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:49,277] Trial 28 finished with value: 0.6463414634146342 and parameters: {'hidden_size': 329, 'dropout_rate': 0.37486406574420983, 'activation': 'Sigmoid', 'optimizer': 'Adam', 'lr': 0.0004713499501179823}. Best is trial 18 with value: 0.6518847006651884.\n",
            "[I 2025-05-18 18:25:52,343] Trial 29 finished with value: 0.5570953436807096 and parameters: {'hidden_size': 202, 'dropout_rate': 0.4139245059386912, 'activation': 'Sigmoid', 'optimizer': 'SGD', 'lr': 0.0011431592052177787}. Best is trial 18 with value: 0.6518847006651884.\n"
          ]
        }
      ],
      "source": [
        "# 訓練に際して、可能であればGPU（cuda）を設定します。GPUが搭載されていない場合はCPUを使用します\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Using {} device\".format(device))\n",
        "\n",
        "# modelを定義します\n",
        "def create_model(trial, input_dim):\n",
        "    # チューニングするハイパーパラメータ\n",
        "    hidden_size = trial.suggest_int(\"hidden_size\", 64, 512)\n",
        "    dropout_rate = trial.suggest_float(\"dropout_rate\", 0.0, 0.5)\n",
        "    activation_name = trial.suggest_categorical(\"activation\", [\"ReLU\", \"Sigmoid\", \"Tanh\"])\n",
        "\n",
        "    # 活性化関数の選択\n",
        "    activation_fn = {\n",
        "        \"ReLU\": nn.ReLU(),\n",
        "        \"Sigmoid\": nn.Sigmoid(),\n",
        "        \"Tanh\": nn.Tanh()\n",
        "    }[activation_name]\n",
        "\n",
        "    # モデル定義（Sequentialで簡素に）\n",
        "    model = nn.Sequential(\n",
        "        nn.Linear(input_dim, hidden_size),\n",
        "        activation_fn,\n",
        "        nn.Dropout(dropout_rate),\n",
        "        nn.Linear(hidden_size, 2)\n",
        "    )\n",
        "\n",
        "    return model.to(device)\n",
        "\n",
        "def objective(trial):\n",
        "    model = create_model(trial, input_dim=X_train_tensor.shape[1])\n",
        "    optimizer_name = trial.suggest_categorical(\"optimizer\", [\"Adam\", \"SGD\"])\n",
        "    lr = trial.suggest_float(\"lr\", 1e-4, 1e-2, log=True)\n",
        "    optimizer = getattr(torch.optim, optimizer_name)(model.parameters(), lr=lr)\n",
        "    loss_fn = nn.CrossEntropyLoss()\n",
        "\n",
        "    for epoch in range(5):  # 短めにして探索時間を抑える\n",
        "        model.train()\n",
        "        for X_batch, y_batch in train_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            optimizer.zero_grad()\n",
        "            pred = model(X_batch)\n",
        "            loss = loss_fn(pred, y_batch)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "    # 検証精度を評価\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    with torch.no_grad():\n",
        "        for X_batch, y_batch in val_loader:\n",
        "            X_batch, y_batch = X_batch.to(device), y_batch.to(device)\n",
        "            pred = model(X_batch)\n",
        "            correct += (pred.argmax(1) == y_batch).sum().item()\n",
        "    accuracy = correct / len(val_loader.dataset)\n",
        "    return accuracy\n",
        "\n",
        "import optuna\n",
        "\n",
        "\n",
        "study = optuna.create_study(direction=\"maximize\")\n",
        "study.optimize(objective, n_trials=30)\n",
        "\n",
        "# 最良のモデルを作成\n",
        "best_model = create_model(study.best_trial, input_dim=X_train_tensor.shape[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c-rtD5ODZLFS",
        "outputId": "5188a29d-295e-498e-aab7-ab7b275702ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0.weight: mean=-2.4553304456276237e-07, std=0.00476008141413331\n",
            "0.bias: mean=-7.246190307341749e-06, std=0.0047655184753239155\n",
            "3.weight: mean=0.0015891260700300336, std=0.026325831189751625\n",
            "3.bias: mean=-0.04101016744971275, std=0.0037996647879481316\n"
          ]
        }
      ],
      "source": [
        "for name, param in best_model.named_parameters():\n",
        "    print(f\"{name}: mean={param.data.mean().item()}, std={param.data.std().item()}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iUhxZFrWPaIB"
      },
      "outputs": [],
      "source": [
        "loss_fn = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.SGD(best_model.parameters(), lr=1e-2)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "71P137ofZj9s"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dnqyJyp9PaIC"
      },
      "outputs": [],
      "source": [
        "def train(dataloader, model, loss_fn, optimizer):\n",
        "    size = len(dataloader.dataset)\n",
        "    for batch, (X, y) in enumerate(dataloader):\n",
        "        X, y = X.to(device), y.to(device)\n",
        "        # 損失誤差を計算\n",
        "        pred = model(X)\n",
        "        #print(pred)\n",
        "        loss = loss_fn(pred, y)\n",
        "\n",
        "        # バックプロパゲーション\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        if batch % 10 == 0:\n",
        "            loss, current = loss.item(), batch * len(X)\n",
        "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OxUT7hOJPaID"
      },
      "outputs": [],
      "source": [
        "def test(dataloader, model):\n",
        "    size = len(dataloader.dataset)\n",
        "    model.eval()\n",
        "    avg_loss, accuracy = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            avg_loss += loss_fn(pred, y).item()\n",
        "            accuracy += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    avg_loss /= size\n",
        "    accuracy /= size\n",
        "    return avg_loss, accuracy\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "epochs = 100\n",
        "patience = 5\n",
        "best_val_loss = float(\"inf\")\n",
        "patience_counter = 0"
      ],
      "metadata": {
        "id": "f-ZBzI79gcvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "4z8ANLM9wvRw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UklgW5aBPaIF"
      },
      "outputs": [],
      "source": [
        "\n",
        "optimizer = torch.optim.Adam(best_model.parameters(), lr=0.001)  # 必要に応じて study.best_params[\"lr\"] を使用\n",
        "\n",
        "for t in range(epochs):\n",
        "    train(train_loader, best_model, loss_fn, optimizer)\n",
        "    val_loss, val_accuracy = test(val_loader, best_model)\n",
        "    print(f\"Val Error: \\n Accuracy: {(100*val_accuracy):>0.1f}%, Val loss: {val_loss:>8f} \\n\")\n",
        "\n",
        "# 最良のモデルを保存&EarlyStopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        patience_counter = 0\n",
        "\n",
        "        best_model_state = copy.deepcopy(best_model.state_dict())\n",
        "    # 検証損失が改善しなかった回数がpatience回を超えたら学習を終了\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "    if patience_counter >= patience:\n",
        "      print(\"Early stopping triggered!\")\n",
        "      break\n",
        "best_model.load_state_dict(best_model_state)\n",
        "print(\"Done!\")\n",
        "\n",
        "# 未知のテストデータでモデル学習、最終的な精度と損失を表示します\n",
        "test_loss, test_accuracy = test(test_loader, best_model)\n",
        "print(f\"Test Error: \\n Accuracy: {(100*test_accuracy):>0.1f}%, Val loss: {test_loss:>8f} \\n\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "S4r6aK92dyHA"
      },
      "outputs": [],
      "source": [
        "y_pred = best_model(X_test_tensor.to(device)).argmax(1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KOq-mg2y-oFH"
      },
      "outputs": [],
      "source": [
        "# テスト用データで予測値を生成する\n",
        "\n",
        "# テスト用データで予測した結果のclassfication_reportを表示する\n",
        "from sklearn.metrics import classification_report\n",
        "\n",
        "# classfication_reportを表示\n",
        "print(classification_report(y_test_tensor.cpu(),y_pred.cpu(),digits=3))\n",
        "\n",
        "# 混同行列を作成してseabornで表示する\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "# 混同行列を表示\n",
        "cm = confusion_matrix(y_test_tensor.cpu(),y_pred.cpu())\n",
        "sns.heatmap(cm, annot=True, cmap='Blues')\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ftPB4qVppx8p"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}